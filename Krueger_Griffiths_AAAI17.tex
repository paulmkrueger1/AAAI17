\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{hyperref}

\usepackage{graphicx}

\usepackage{titling}
\graphicspath{{figures/final/}}


\pdfinfo{
/Title (Shaping Model-Free Reinforcement Learning with Model-Based Pseudorewards)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%

\title{Shaping Model-Free Reinforcement Learning with Model-Based Pseudorewards}
\date{}
\maketitle
\begin{abstract}
Model-free and model-based reinforcement learning have provided a successful framework for understanding human behavior and brain data. These two systems are usually thought to compete for control of behavior. However, models exist which integrate the two in a cooperative manner. Dyna, and its variant, prioritized sweeping, use model-based replay of past experience to train the model-free system. Here we introduce a new method which links the two systems via the reward function. Dynamic programming is used to iteratively estimate state values that monotonically converge to state values under the optimal policy. \textit{Pseudorewards} are calculated from these values and used to shape the reward function in a way that is guaranteed not to change the optimal policy. In two experiments we show that this method offers computational advantages over Dyna and prioritized sweeping. It also offers a new way to think about integrating model-free and model-based reinforcement learning. One interesting hypothesis is that emotions may in certain conditions function as model-based pseudorewards that tune a model-free system.
\end{abstract}

\section{Introduction}

Researchers in both psychology and artificial intelligence taken the relationship between model-free and model-based reinforcement learning, and whether they can be used synergistically. Model-free learning relies on direct trial-and-error interaction with the environment\~cite{sutton1992reinforcement}, while model-based learning occurs through simulation of knowledge about the causal structure of the environment~\cite{barto1995learning}. Historically, animal psychologists viewed these two systems as distinct and competing hypotheses, with behaviorists arguing in favor of reflexive, model-free learning based on stimulus-response associations~\cite{thorndike1933proof}, and Tolman and others positing an internal representation of the environment, referred to as the "cognitive map"~\cite{tolman1948cognitive}. Nowadays, while both behavioral and neural data indicate that human learning relies on both systems (e.g.~\cite{daw2005uncertainty},~\cite{glascher2010states},~\cite{dayan2014model}), it is typically assumed that they only compete for control of behavior. Yet it is also possible for both systems to cooperate. The Dyna architecture achieves such cooperation by integrating model-free learning with model-based planning~\cite{sutton1991dyna}. In Dyna, as model-free learning occurs, state-action transitions and reward contingencies are cached into a model, which simultaneously replays these past experiences, using them to further train model-free state-action values.

Here we introduce a new method for cooperative interaction between model-free and model-based learning. The model-based system generates pseudorewards that are used to shape the model-free reward function. The idea of modifying the reward function to improve behavior is central to \textit{gamification}, in which additional rewards or incentives---\textit{pseudorewards}---are used to influence behavior~\cite{mcgonigal2011reality}. Moreover, according to the \textit{shaping theorem}, conditions exist under which the optimal policy for a Markov Decision Process will remain invariant to such modifications of the reward function, opening the possibility that pseudorewards can be used to guide agents toward optimal behavior~\cite{ng1999policy}. That is, if the optimal policy can be guaranteed to remain unchanged with the introduction of pseudorewards, then pseudorewards can potentially be used to guide the agent to the optimal policy. Using these principles, we show that pseudorewards can provide a link between model-free and model-based learning through modification of the reward function.

This method of cooperation between learning systems offers an appealing alternative to Dyna both conceptually and practically. Dyna uses model-based replay of past experience to refine state-action values, and can be conceptualized in a cognitive framework as using memory replay or planning to train model-free learning. Recent behavioral data with humans performing a retrospective revaluation task is consistent with a cooperative archetecture like Dyna. Our method introduced here links the two systems cooperatively by shaping the reward function. Cognitively, one way that such shaping might occur is with emotion. Emotions could be produced based on a model of the environment and used as pseudorewards to tune the habitual model-free system. To give a concrete example, one might have knowledge of a normative model of ethics, and when one reactively behaves in a way that violates the model, the feeling of remorse might be generated. This feeling could function as a pseudoreward by reshaping the reward function, making the person less likely to misbehave in the future. Our method also offers a practical advantage to Dyna by requiring less computation time and learning more quickly.

In this paper we begin by reviewing the Dyna architecture for integrated model-free and model-based learning. We then introduce our method and the theoretical background on which it is based. We present two experiments which show the effectiveness of our model, and how it compares to Dyna. The first experiment involves learning in a maze environment, and the second experiment uses the standard mountain car problem. We end by discussing additional variants of Dyna and our method, and consider how this integrated approach might provide a useful model for understanding human cognition and emotion.

\section{Cooperative Reinforcement Learning}

Dyna uses a model-based system to replay past experiences, which are used to train the model-free system (Figure~\ref{dyna_schematic}). After each action taken in the environment, the model stores the state-action pair and reward received. It then randomly selects $n$ past state-action pairs and replays them, using them to update the model-free system as if they were real actions. In Dyna-Q, the model-free system uses one-step tabular Q-learning (which is what we use in our experiments).The number of simulated planning steps, $n$, is a parameter that can be set to any positive integer value. Dyna typically begins with no knowledge about the causal structure of the environment (i.e. transitions between states and reward contingencies), but builds this knowledge based on experience. However, Dyna can also inherit a model of the environment, and we discuss this possibility later.

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{dyna_schematic}
\caption{Schematic of the Dyna archetecture}
\label{fig:dyna_schematic}
\end{figure}

In addition to being a useful artificial intelligence agent for integrating direct learning with indirect replay, it has begun to provide a useful model of human cognition. ~\cite{gershman2014retrospective} found behavioral evidence in humans cositent with a Dyna archetecture.  Participants performed a sequential decision task with separate phases that tested behavioral revaluatation. When given either more time between phases of learning or a smaller cognitive load, the magnitude of revaluation was larger, consistent with model-based replay of past experience. There are also neurophysiological data that suggest Dyna-like cooperation between the two systems. ~\cite{lansink2009hippocampus} identified cells in the hippocampus of rats encoding spatial location and cells in the striatum of the same rats that encoded reward. During sleep, the activation of hippocampal cells correlated with and proceeded activation of the same striatal cells that encoded the value of those locations.

\section{Model-based pseudoreward approximation}

Our method uses dynamic programming to approximate state values. These values are used to calculate pseudorewards according to the shaping theorem. By shaping the reward function, pseudorewards provide a link between model-based planning and model-free learning.

\subsection{Pseudorewards}

Pseudorewards are an intelligible way of conferring extra information to an agent about the reward landscape. Essentially, a small reward is given to the model-free agent (a Q-learner in our experiments) whenever it takes an action that helps the agent move towards the goal. Instead of the agent receiving actual reward $R(s, s')$ when moving from state $s \rightarrow s'$, the agent receives an augmented reward $R'(s, s')$ where
\begin{equation}
R'(s, s') = R(s, s') + F(s, s')
\end{equation} 

\subsection{Shaping theorem}

Pseudorewards are defined by \textit{shaping functions} $F$.  In~\cite{ng1999policy}, conditions for which the optimal policy $\pi^*$ remains invariant under a \textit{shaping function} are developed. If the shaping function does not possess this invariance policy, it is possible that Q-learning will converge to a suboptimal solution. The simplest example of an invariant shaping function uses the difference in optimal values between the agent's current state and next state:
\begin{equation}
F(s, s') = \gamma V_{\pi^*}(s') - V_{\pi^*}(s) 
\end{equation}
\begin{equation}
V_{\pi^*}(s) =  \max_{a} R(s, s') + \gamma V_{\pi^*}(s')
\end{equation}
This method is called the \textit{optimal policy pseudoreward}--it encourages the agent to always move down the optimal path from its current state. If $\epsilon = 0$, the agent would move directly to the goal along the shortest path with an $\epsilon$-greedy decision policy.

With \textit{optimal policy pseudorewards} the agent can maximze long-term reward simply by taking the most reward action at each step. In real-world scenarios, it's often unrealistic for a human to have such a complete information set. Computing the optimal policy usually entails solving a linear program.

\subsection{Bouded-RTDP}

Bounded Real-Time Dynamic Programming~\cite{mcmahan2005bounded} is a planning algorithm that attains certain performance guarantees if its lower and upper bounded estimates of state values converge monotonically toward state values under the optimal policy. Importantly, this monotonic convergence toward optimal values can be proved to occur if the lower and upper bounds are initialized properly. Here, we take advantage of this monotone property to calculate approximate (but progressively better) state values using dynamic programming. These values are then used to approximate pseudorewards according to the shaping theorem.

Figure~\ref{fig:approxPR_schematic}  provides a schematic illustration of how dynamic programming is used to approximate pseudorewards, which in turn shape the reward function and policy of the model-free agent.

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{approxPR_schematic}
\caption{Schematic of the approximate pseudoreward shaping method}
\label{fig:approxPR_schematic}
\end{figure}

\section{Experiment 1}

\subsection{Methods}

\textbf{Maze learning} The agent (a simple Q-learner), began each episode in the upper-left corner of a maze (Figure~\ref{fig:maze_values}), and was rewarded one point for reaching the lower-right corner (Figure~\ref{fig:maze_values}). The state space consisted of 121 locations in the grid shown in Figure~\ref{fig:maze_values}, and actions consisted of each of the four cardinal directions. The agent was trained for fifty episodes, with each episode ending when the goal was reached, or 2,000 steps were taken (whichever came first). An $\epsilon$-greedy decision policy was used with $\epsilon = 0.25$. The colors in Figure~\ref{fig:maze_values} correspond to state values under the optimal policy. Since rewards were discounted with $\gamma = 0.95$, the value of each state is $0.95^{\min{stepsToGoal}}$. All simulations were run ten times and averaged to produce plots.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{maze_values}
\caption{Maze environment. Colors correspond to state values under the optimal policy. (S=start state, G=goal state)}
\label{fig:maze_values}
\end{figure}

\noindent
\textbf{Approximate pseudorewards} Dynamic programing was used to approximate state values by iterating over the Bellman equation. In~\cite{mcmahan2005bounded} the authors detail conditions under which initial state values will provably converge monotonically toward optimal values, but they note that in practice most reasonable initial values will achieve this monotonic convergence. Here, all states were initialized with a lower bound of zero and an upper bound of one, which, in our simple environment, is known to bound state values. Figure~\ref{fig:value_bounds} shows that the approximate state values do indeed converge monotonically. The point at which each state reaches its optimal value is exactly equal to the minimum number of steps that state is away from the goal.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{value_bounds_labeled}
\caption{Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. (Red lines=upper bound estimate, blue lines=lower bound estimate, dashed lines=optimal state values)}
\label{fig:value_bounds}
\end{figure}

At each state, the pseudoreward for each action was calculated according to the shaping theorem as the difference between the value of the current state and the value of the of the next state given that action (which was deterministic in this environment). Pseudorewards are adding onto actual rewards to shape the reward function, thereby guiding the Q-learning algorithm.

\noindent
\textbf{Tradeoff between model-free and model-based computation} The closer pseudorewards are to their optimal values, the easier the learning for the model-free agent (at least to some precision). However, whereas Q-learning is simple and quick, the model-based method of approximating state values is relatively slow and computationally costly. Therefore, we sought to understand the most efficient tradeoff between model-based pseudoreward approximation and model-free learning. This was done by computing the amount of CPU time (in seconds) required for each algorithm (while CPU time is a variable factor, the relative time across algorithms is generally invariant).

\subsection{Results}

Either the lower bound of state values or the upper bound (or the mean of the two) after $n$ iterations of Bellman updates can be used to approximate pseudorewards for Q-learning, according to the shaping theorem. Figure~\ref{fig:maze1} shows the number of steps per episode needed to reach the goal, averaged across 50 episodes, as a function of the the number of Bellman updates used to approximate pseudorewards. The red line shows learning when approximate pseudorewards are based on upper bound state values, the blue line is for lower bound values, and the black line is for pseudorewards that are the mean of the two. As expected, learning is quicker when pseudorewards are closer to their optimal values. For comparison, we also show performance of the Dyna and prioritized sweeping algorithms, as a function of the number of planning steps taken after each real step (separate green x-axis). While approximate pseudorewards are calculated just once using $n$ iterations, the planning steps used by Dyna and prioritized sweeping are taken \textit{after every single step of every episode}. The dashed green lines show the number of real steps and the number of planning steps taken using Dyna.

Because Dyna and prioritized sweeping must learn a model of the environment through experience, the first episodes require more than the minimal number of steps to learn the goal, which is why the average across episodes is higher than the shortest path (34 steps when $\epsilon=0.25$). With sufficiently precise pseudorewards, on the other hand, the agent can learn the shortest path on the very first episode. Specifically, 24 Bellman updates are required for this, because the start state is 24 steps away from the goal state; after 24 iterations of the Bellman equation, optimal state values have propogated back from the goal state to the start state, along the shortest path.

Also shown are the number of steps taken by a simple Q-learning agent when states values are initialized to 0 (blue asterisk), 1 (red asterisk), or 0.5 (black asterisk).

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{learning_vs_PRiterations_DYNA_mean}
\caption{Pseudoreward approximation requires fewer steps to reach the goal than Dyna or prioritized sweeping. The number of steps taken to reach the goal, averaged across 50 episodes, is displayed as a function of the number of iterations of pseudoreward approximation, or the number of planning steps used by Dyna. With a sufficient number of iterations of pseudoreward approximation, the agent will reach the goal in the minimum number of steps on the very first episode. Dyna will always require some training episodes before achieving this, which is why the average is higher.}
\label{fig:maze1}
\end{figure}

Next, we calculated the actual time (in CPU seconds) required to learn the shortest path. While the pseudoreward method may take fewer steps to reach the goal than either Dyna or prioritized sweeping, it does not necessarily mean its faster. Planning steps are about two orders of magnitude quicker than Bellman updates, so it's possible that Dyna is still faster. However, figure~\ref{fig:maze2} shows that pseudoreward approximation is still faster than Dyna. The fastest learning occurs when 24 iterations of the Bellman equation are used; any more than this is unnecessary and the CPU time gradually increases linearly. Prioritized sweeping is not shown because it takes at least an order of magnitude longer to run, due to maintaining a sorted list of the most informative states.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{cpus_vs_PRiterations_DYNA_toGoal}
\caption{Pseudoreward approximation learns the shortest path more quickly than Dyna.}
\label{fig:maze2}
\end{figure}

\section{Experiment 2}

\subsection{Methods}

\textbf{Mountain cart problem} Experiment 2 entailed learning in a classic mountain car environment. The agent begins in a valley between two mountains with the goal of reaching the star at the top of the mountain on the right (figure~\ref{fig:mc}). The agent must learn to apply force such that it oscillates between the slopes of each mountain, building enough momentum to eventually reach the top. States consisted of discretized locations along the 2-D mountainsides and discretized velocities. Actions consisted of discretized forces applied tangentially to the direction of movement. The agent used Q-learning during 200 learning episodes, where each episode ended when the car reached the goal state (or if 1,000 steps were taken). For every state-action not leading to the goal, the agent was punished one point, and was awarded 100 points for reaching the goal. An $\epsilon$-greedy decision policy was used where $\epsilon=0.01\times0.99^{episodeNumber-1}$. As before, all simulations were run ten times and averaged together.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{Mcar}
\caption{The mountain car problem.}
\label{fig:mc}
\end{figure}

\noindent
\textbf{Comparison of learning methods} As before, pseudorewards were approximated using bounded dynamic programming and the shaping theorem. Performance using this algorithm was compared with Dyna.

\subsection{Results}

Figure~\ref{fig:mc1} shows the number of steps per episodes required to reach the goal, averaged across 200 episodes. Although not shown, the upper-bound and lower-bound estimates of state values all converged to optimal values after 73 iterations of the Bellman equation. 73 is the number of steps required to reach the goal from the starting state under the optimal policy, and thus after 73 Bellman updates, values from the goal state had propagated back to the start state (in this task the start state is the furthest possible state away from the goal). The x-axis shows the number of Bellman updates used to estimate pseudorewards. As before, the blue line indicates pseudorewards based on the lower-bound estimation of state values the red line is for the upper-bound approximation of state values, and the black line is using pseudorewards based on the mean of the upper bound and lower bound state values. The green lines show the performance of Dyna. The total number of steps (real steps plus simulated steps) far exceeds the number of steps using our method, and even the number of actual steps alone does not converge as low as the number of steps taken using our method. The blue asterisk indicates performance of a simple Q-learning agent.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{MC_learning_vs_PRiterations_DYNA_mean}
\caption{Performance of the pseudoreward approximation method during the mountain car problem. The average number of steps taken across episodes is plotted against the number of model-based iterations of pseudoreward approximation. Also shown (green lines) is performance of Dyna.}
\label{fig:mc1}
\end{figure}

Figure~\ref{fig:mc2} shows the amount of time in CPU seconds required to learn the shortest path. Although Dyna requires many more steps to learn, because its computations are scalar-based Q-learning approximations, it is relatively quick, whereas Bellman approximation requires more costly matrix multiplication. Still, the pseudoreward approximation method learns more quickly.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{MC_cpus_vs_PRiterations_DYNA_toGoal}
\caption{The amount of CPU time required to learn the shortest path.}
\label{fig:mc2}
\end{figure}

\section{Discussion}

We have introduced a new method for cooperatively integrating model-free and model-based reinforcement learning. This method relies on bounded dynamic programming as a model-based method to iteratively estimate state values that converge monotonically to values under the optimal policy. These approximate values are used to calculate pseudorewards according to the shaping theorem, such that the reward function is altered but the optimal policy is invariant. This reward function is used for model-free learning. Our experiments demonstrate that this method performs comparably and even better than the Dyna algorithm, which is another cooperative reinforcement learning method.

One important difference between our method and Dyna is that Dyna learns the model of the environment, whereas our model is omniscient (that is, it is given the full state-action transition matrix and reward-action pairs). This may at first make any comparison between the two methods in terms of learning or computation time unfair. In actuality, however, if Dyna is given a full model so that it is equally omniscient, learning and and computation performance goes a tiny bit down in the maze environment, and improves marginally in the mountain car task. An omniscience Dyna agent will only learning more quickly during the first episode, before it has discovered the location of reward. Once it knows how to reach the goal, a full model is unnecessary, and would even slow learning through planning, because unvisited states that do not help the agent reach the goal will be replayed. One modification that would save computation time for Dyna would be to modulate the number of planning steps in proportion to the change in variance of estimated Q-values. When the temporal difference errors on average are larger, more planning steps are needed, but as they converge, the number of planning steps would go down. Another improvement to Dyna, which has been studied, is known as prioritized sweeping~\cite{moore1993prioritized}. Here, the replay of state-action pairs is selected from the top of a que, where the que is sorted by the temporal difference error from that state action pair. This allows states with the most uncertainty about their value to be replayed the most, and the effect of this is that learning of state values propagates backwards from rewarding states. If a prioritized sweeping algorithm were omniscient in the sense described above, then the number of planning steps needed to compute Q-values that would allow the agent to follow an optimal policy would simply be the distance from the start state to the goal state. While prioritized sweeping would require fewer steps (real and simulated) than a Dyna to learn the shortest path, the sorting the que after every simulated step, as described above, is extremely costly in terms of CPU time, and therefore takes much longer to compute in practice.

While our method is omniscient with respect to state-action transitions and rewards, this need not be the case. Our method can also be initialized with a naive model that has a uniform prior for all transition probabilities and reward outcomes (that is, any action is assumed to transition to any other state with equal probability and the expected reward for any action is $R/(number of states)$, where $R$ is the expected reward for (optimally) performing the task). This model can be used for state value approximation with bounded RTDP, just as before, and as experience is acquired, the model can be updated (with either probabilistic transition estimates or deterministic ones if the environment is assumed to be such). When our method is run this way, however, the change in performance in our environments is negligible. When the model is initialized with a uniform prior on rewards, the expected reward for any given action is relatively small. As a result, as soon as the agent discovers (relatively large) actual reward, this has a much bigger impact on estimated state values.

By providing a new way to link model-free and model-based reinforcement learning, our method offers a new way to think about human cognition, and potentially test experimental. While experimental findings on cooperative reinforcement learning in humans is nascent, there is substantial interest in understanding the interactions between them~\cite{daw2014algorithmic}. As discussed earlier, Dyna is readily likened to memory replay (including planning) in human cognition as a means to train a model-free system. What might be an analogue of pseudoreward approximation in human cognition? For any given task or goal-directed behavior, emotions quite often have the effect of altering the reward landscape, and it is reasonable to think of them as pseudorewards when the goal itself is something external or independent of the emotion. If certain emotions represent the values of states that are stored in a model, and these emotions are used to train model-free learning by adding bonuses (positive emotions) or punishments (negative emotions) to certain actions, this would be quite akin to our method. The accuracy with which the emotion represents the value of a state would depend on the accuracy of the model, and could be implemented using something similar to Bellman approximation. In the introduction, we used the example of the feeling of remorse based on a model of ethics as a means to train the habitual learning system to avoid such future actions. It is worth pursuing these questions experimentally to test the utility of our method for understanding human cognition.

\bibliography{Krueger_Griffiths_AAAI17}{}
\bibliographystyle{apalike}

\end{document}
