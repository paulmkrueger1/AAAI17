\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{barto1995learning}
\citation{thorndike1933proof}
\citation{tolman1948cognitive}
\citation{daw2005uncertainty}
\citation{glascher2010states}
\citation{dayan2014model}
\citation{sutton1991dyna}
\citation{mcgonigal2011reality}
\citation{ng1999policy}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}}
\citation{gershman2014retrospective}
\citation{lansink2009hippocampus}
\citation{ng1999policy}
\citation{mcmahan2005bounded}
\@writefile{toc}{\contentsline {section}{Cooperative Reinforcement Learning}{2}{section*.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic of the Dyna archetecture\relax }}{2}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dyna_schematic}{{1}{2}{Schematic of the Dyna archetecture\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{Model-based pseudoreward approximation}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{Pseudorewards}{2}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{Shaping theorem}{2}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{Bouded-RTDP}{2}{section*.7}}
\@writefile{toc}{\contentsline {section}{Experiment 1}{2}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{Methods}{2}{section*.10}}
\citation{mcmahan2005bounded}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic of the approximate pseudoreward shaping method\relax }}{3}{figure.caption.8}}
\newlabel{fig:approxPR_schematic}{{2}{3}{Schematic of the approximate pseudoreward shaping method\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Maze environment. Colors correspond to state values under the optimal policy. (S=start state, G=goal state)\relax }}{3}{figure.caption.11}}
\newlabel{fig:maze_values}{{3}{3}{Maze environment. Colors correspond to state values under the optimal policy. (S=start state, G=goal state)\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. (Red lines=upper bound estimate, blue lines=lower bound estimate, dashed lines=optimal state values)\relax }}{3}{figure.caption.12}}
\newlabel{fig:value_bounds}{{4}{3}{Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. (Red lines=upper bound estimate, blue lines=lower bound estimate, dashed lines=optimal state values)\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{Results}{3}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Pseudoreward approximation requires fewer steps to reach the goal than Dyna or prioritized sweeping. The number of steps taken to reach the goal, averaged across 50 episodes, is displayed as a function of the number of iterations of pseudoreward approximation, or the number of planning steps used by Dyna. With a sufficient number of iterations of pseudoreward approximation, the agent will reach the goal in the minimum number of steps on the very first episode. Dyna will always require some training episodes before achieving this, which is why the average is higher.\relax }}{4}{figure.caption.14}}
\newlabel{fig:maze1}{{5}{4}{Pseudoreward approximation requires fewer steps to reach the goal than Dyna or prioritized sweeping. The number of steps taken to reach the goal, averaged across 50 episodes, is displayed as a function of the number of iterations of pseudoreward approximation, or the number of planning steps used by Dyna. With a sufficient number of iterations of pseudoreward approximation, the agent will reach the goal in the minimum number of steps on the very first episode. Dyna will always require some training episodes before achieving this, which is why the average is higher.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pseudoreward approximation learns the shortest path more quickly than Dyna.\relax }}{4}{figure.caption.15}}
\newlabel{fig:maze2}{{6}{4}{Pseudoreward approximation learns the shortest path more quickly than Dyna.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{Experiment 2}{4}{section*.16}}
\@writefile{toc}{\contentsline {subsection}{Methods}{4}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{Results}{4}{section*.19}}
\citation{moore1993prioritized}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The mountain car problem.\relax }}{5}{figure.caption.18}}
\newlabel{fig:mc}{{7}{5}{The mountain car problem.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance of the pseudoreward approximation method during the mountain car problem. The average number of steps taken across episodes is plotted against the number of model-based iterations of pseudoreward approximation. Also shown (green lines) is performance of Dyna.\relax }}{5}{figure.caption.20}}
\newlabel{fig:mc1}{{8}{5}{Performance of the pseudoreward approximation method during the mountain car problem. The average number of steps taken across episodes is plotted against the number of model-based iterations of pseudoreward approximation. Also shown (green lines) is performance of Dyna.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The amount of CPU time required to learn the shortest path.\relax }}{5}{figure.caption.21}}
\newlabel{fig:mc2}{{9}{5}{The amount of CPU time required to learn the shortest path.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{Discussion}{5}{section*.22}}
\citation{daw2014algorithmic}
\bibdata{Krueger_Griffiths_AAAI17}
\bibcite{barto1995learning}{Barto et\nobreakspace  {}al., 1995}
\bibcite{daw2005uncertainty}{Daw et\nobreakspace  {}al., 2005}
\bibcite{dayan2014model}{Dayan and Berridge, 2014}
\bibcite{gershman2014retrospective}{Gershman et\nobreakspace  {}al., 2014}
\bibcite{glascher2010states}{Gl{\"a}scher et\nobreakspace  {}al., 2010}
\bibcite{lansink2009hippocampus}{Lansink et\nobreakspace  {}al., 2009}
\bibcite{mcgonigal2011reality}{McGonigal, 2011}
\bibcite{mcmahan2005bounded}{McMahan et\nobreakspace  {}al., 2005}
\bibcite{ng1999policy}{Ng et\nobreakspace  {}al., 1999}
\bibcite{sutton1991dyna}{Sutton, 1991}
\bibcite{thorndike1933proof}{Thorndike, 1933}
\bibcite{tolman1948cognitive}{Tolman, 1948}
\bibstyle{apalike}
