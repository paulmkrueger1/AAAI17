\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutton1992reinforcement}
\citation{barto1995learning}
\citation{thorndike1933proof}
\citation{tolman1948cognitive}
\citation{daw2005uncertainty,glascher2010states,dayan2014model}
\citation{sutton1991dyna}
\citation{gershman2014retrospective}
\citation{ng1999policy}
\citation{sutton1998reinforcement}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{Cooperative Reinforcement Learning}{1}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{Markov Decision Processes}{1}{section*.3}}
\citation{sutton1998reinforcement}
\@writefile{toc}{\contentsline {subsection}{Model-free Reinforcement Learning}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{Model-based Reinforcement Learning}{2}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{Dyna}{2}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic of the Dyna archetecture.\relax }}{2}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dyna_schematic}{{1}{2}{Schematic of the Dyna archetecture.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{Model-based pseudoreward approximation}{2}{section*.8}}
\citation{ng1999policy}
\citation{mcmahan2005bounded}
\citation{sutton1991dyna,sutton1991planning,peng1993efficient,sutton1998reinforcement,wiering2012reinforcement}
\citation{mcmahan2005bounded}
\@writefile{toc}{\contentsline {subsection}{Pseudorewards and the shaping theorem}{3}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{Approximating the value function}{3}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{Linking model-free and model-based RL with the reward function}{3}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The approximate pseudoreward shaping method.\relax }}{3}{figure.caption.12}}
\newlabel{fig:approxPR_schematic}{{2}{3}{The approximate pseudoreward shaping method.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{Experiment 1: Maze learning}{3}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{Methods}{3}{section*.14}}
\citation{moore1990efficient,sutton1996generalization,sutton1998reinforcement,smart2000practical,rasmussen2003gaussian,whiteson2006evolutionary,heidrich2008variable,sutton2012dyna}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Maze environment. Colors correspond to state values under the optimal policy, S is start state, and G is goal state.\relax }}{4}{figure.caption.15}}
\newlabel{fig:maze_values}{{3}{4}{Maze environment. Colors correspond to state values under the optimal policy, S is start state, and G is goal state.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{Results}{4}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. Red lines are upper-bound estimates, blue lines are lower-bound estimate, and dashed lines are optimal state values.\relax }}{4}{figure.caption.16}}
\newlabel{fig:value_bounds}{{4}{4}{Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. Red lines are upper-bound estimates, blue lines are lower-bound estimate, and dashed lines are optimal state values.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{Experiment 2: Mountain car problem}{4}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{Methods}{4}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Pseudoreward approximation requires fewer steps to reach the goal than Dyna during maze learning.\relax }}{5}{figure.caption.18}}
\newlabel{fig:maze1}{{5}{5}{Pseudoreward approximation requires fewer steps to reach the goal than Dyna during maze learning.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Performance during the mountain car problem.\relax }}{5}{figure.caption.22}}
\newlabel{fig:mc1}{{7}{5}{Performance during the mountain car problem.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pseudoreward approximation learns the shortest path more quickly than Dyna with maze learning.\relax }}{5}{figure.caption.19}}
\newlabel{fig:maze2}{{6}{5}{Pseudoreward approximation learns the shortest path more quickly than Dyna with maze learning.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{Results}{5}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces CPU time required to learn the shortest path.\relax }}{5}{figure.caption.24}}
\newlabel{fig:mc2}{{8}{5}{CPU time required to learn the shortest path.\relax }{figure.caption.24}{}}
\citation{moore1993prioritized}
\citation{daw2014algorithmic}
\citation{cushman2013action,crockett2013models}
\bibdata{model-free-model-based-shaping}
\bibcite{barto1995learning}{Barto et\nobreakspace  {}al., 1995}
\@writefile{toc}{\contentsline {section}{Discussion}{6}{section*.25}}
\@writefile{toc}{\contentsline {subsection}{Equalizing knowledge}{6}{section*.26}}
\@writefile{toc}{\contentsline {subsection}{Learning the model}{6}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{Prioritized sweeping}{6}{section*.28}}
\@writefile{toc}{\contentsline {subsection}{Pseudorewards and emotion}{6}{section*.29}}
\bibcite{crockett2013models}{Crockett, 2013}
\bibcite{cushman2013action}{Cushman, 2013}
\bibcite{daw2014algorithmic}{Daw and Dayan, 2014}
\bibcite{daw2005uncertainty}{Daw et\nobreakspace  {}al., 2005}
\bibcite{dayan2014model}{Dayan and Berridge, 2014}
\bibcite{gershman2014retrospective}{Gershman et\nobreakspace  {}al., 2014}
\bibcite{glascher2010states}{Gl{\"a}scher et\nobreakspace  {}al., 2010}
\bibcite{heidrich2008variable}{Heidrich-Meisner and Igel, 2008}
\bibcite{mcmahan2005bounded}{McMahan et\nobreakspace  {}al., 2005}
\bibcite{moore1990efficient}{Moore, 1990}
\bibcite{moore1993prioritized}{Moore and Atkeson, 1993}
\bibcite{ng1999policy}{Ng et\nobreakspace  {}al., 1999}
\bibcite{peng1993efficient}{Peng and Williams, 1993}
\bibcite{rasmussen2003gaussian}{Rasmussen et\nobreakspace  {}al., 2003}
\bibcite{smart2000practical}{Smart and Kaelbling, 2000}
\bibcite{sutton1991dyna}{Sutton, 1991a}
\bibcite{sutton1991planning}{Sutton, 1991b}
\bibcite{sutton1996generalization}{Sutton, 1996}
\bibcite{sutton1998reinforcement}{Sutton and Barto, 1998}
\bibcite{sutton1992reinforcement}{Sutton et\nobreakspace  {}al., 1992}
\bibcite{sutton2012dyna}{Sutton et\nobreakspace  {}al., 2012}
\bibcite{thorndike1933proof}{Thorndike, 1933}
\bibcite{tolman1948cognitive}{Tolman, 1948}
\bibcite{whiteson2006evolutionary}{Whiteson and Stone, 2006}
\bibcite{wiering2012reinforcement}{Wiering and Van\nobreakspace  {}Otterlo, 2012}
\bibstyle{aaai}
