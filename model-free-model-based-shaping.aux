\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{barto1995learning}
\citation{thorndike1933proof}
\citation{tolman1948cognitive}
\citation{daw2005uncertainty,glascher2010states,dayan2014model}
\citation{sutton1991dyna}
\citation{gershman2014retrospective}
\citation{ng1999policy}
\citation{cushman2013action,crockett2013models}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}}
\citation{sutton1998reinforcement}
\citation{sutton1998reinforcement}
\citation{gershman2014retrospective}
\citation{lansink2009hippocampus}
\@writefile{toc}{\contentsline {section}{Cooperative Reinforcement Learning}{2}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{Markov Decision Processes}{2}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{Model-free Reinforcement Learning}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{Model-based Reinforcement Learning}{2}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{Dyna}{2}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic of the Dyna archetecture\relax }}{2}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dyna_schematic}{{1}{2}{Schematic of the Dyna archetecture\relax }{figure.caption.7}{}}
\citation{ng1999policy}
\citation{mcmahan2005bounded}
\citation{sutton1991dyna,sutton1991planning,peng1993efficient,sutton1998reinforcement,wiering2012reinforcement}
\@writefile{toc}{\contentsline {section}{Model-based pseudoreward approximation}{3}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{Pseudorewards}{3}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{Shaping theorem}{3}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{Approximating the value function with Bounded RTDP}{3}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic of the approximate pseudoreward shaping method\relax }}{3}{figure.caption.12}}
\newlabel{fig:approxPR_schematic}{{2}{3}{Schematic of the approximate pseudoreward shaping method\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{Experiment 1: Maze learning}{3}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{Methods}{3}{section*.14}}
\citation{mcmahan2005bounded}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Maze environment. Colors correspond to state values under the optimal policy. (S=start state, G=goal state)\relax }}{4}{figure.caption.15}}
\newlabel{fig:maze_values}{{3}{4}{Maze environment. Colors correspond to state values under the optimal policy. (S=start state, G=goal state)\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. (Red lines=upper bound estimate, blue lines=lower bound estimate, dashed lines=optimal state values)\relax }}{4}{figure.caption.16}}
\newlabel{fig:value_bounds}{{4}{4}{Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. (Red lines=upper bound estimate, blue lines=lower bound estimate, dashed lines=optimal state values)\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{Results}{4}{section*.17}}
\citation{moore1990efficient,sutton1996generalization,sutton1998reinforcement,smart2000practical,rasmussen2003gaussian,whiteson2006evolutionary,heidrich2008variable,sutton2012dyna}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Pseudoreward approximation requires fewer steps to reach the goal than Dyna. The number of steps taken to reach the goal, averaged across 50 episodes, is displayed as a function of the number of iterations of pseudoreward approximation, or the number of planning steps used by Dyna. With a sufficient number of iterations of pseudoreward approximation, the agent will reach the goal in the minimum number of steps on the very first episode.\relax }}{5}{figure.caption.18}}
\newlabel{fig:maze1}{{5}{5}{Pseudoreward approximation requires fewer steps to reach the goal than Dyna. The number of steps taken to reach the goal, averaged across 50 episodes, is displayed as a function of the number of iterations of pseudoreward approximation, or the number of planning steps used by Dyna. With a sufficient number of iterations of pseudoreward approximation, the agent will reach the goal in the minimum number of steps on the very first episode.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{Experiment 2: Mountain car problem}{5}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{Methods}{5}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pseudoreward approximation learns the shortest path more quickly than Dyna.\relax }}{5}{figure.caption.19}}
\newlabel{fig:maze2}{{6}{5}{Pseudoreward approximation learns the shortest path more quickly than Dyna.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The mountain car problem.\relax }}{5}{figure.caption.22}}
\newlabel{fig:mc}{{7}{5}{The mountain car problem.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{Results}{5}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance of the pseudoreward approximation method during the mountain car problem. The average number of steps taken across episodes is plotted against the number of model-based iterations of pseudoreward approximation. Also shown (green lines) is performance of Dyna.\relax }}{6}{figure.caption.24}}
\newlabel{fig:mc1}{{8}{6}{Performance of the pseudoreward approximation method during the mountain car problem. The average number of steps taken across episodes is plotted against the number of model-based iterations of pseudoreward approximation. Also shown (green lines) is performance of Dyna.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{Discussion}{6}{section*.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The amount of CPU time required to learn the shortest path.\relax }}{6}{figure.caption.25}}
\newlabel{fig:mc2}{{9}{6}{The amount of CPU time required to learn the shortest path.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{Equalizing knowledge}{6}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{Learning the model}{6}{section*.28}}
\citation{moore1993prioritized}
\citation{daw2014algorithmic}
\citation{cushman2013action,crockett2013models}
\bibdata{model-free-model-based-shaping}
\bibcite{barto1995learning}{Barto et\nobreakspace  {}al., 1995}
\bibcite{crockett2013models}{Crockett, 2013}
\bibcite{cushman2013action}{Cushman, 2013}
\bibcite{daw2014algorithmic}{Daw and Dayan, 2014}
\bibcite{daw2005uncertainty}{Daw et\nobreakspace  {}al., 2005}
\bibcite{dayan2014model}{Dayan and Berridge, 2014}
\bibcite{gershman2014retrospective}{Gershman et\nobreakspace  {}al., 2014}
\bibcite{glascher2010states}{Gl{\"a}scher et\nobreakspace  {}al., 2010}
\bibcite{heidrich2008variable}{Heidrich-Meisner and Igel, 2008}
\bibcite{lansink2009hippocampus}{Lansink et\nobreakspace  {}al., 2009}
\bibcite{mcmahan2005bounded}{McMahan et\nobreakspace  {}al., 2005}
\@writefile{toc}{\contentsline {subsection}{Prioritized sweeping}{7}{section*.29}}
\@writefile{toc}{\contentsline {subsection}{Pseudorewards and emotion}{7}{section*.30}}
\bibcite{moore1990efficient}{Moore, 1990}
\bibcite{moore1993prioritized}{Moore and Atkeson, 1993}
\bibcite{ng1999policy}{Ng et\nobreakspace  {}al., 1999}
\bibcite{peng1993efficient}{Peng and Williams, 1993}
\bibcite{rasmussen2003gaussian}{Rasmussen et\nobreakspace  {}al., 2003}
\bibcite{smart2000practical}{Smart and Kaelbling, 2000}
\bibcite{sutton1991dyna}{Sutton, 1991a}
\bibcite{sutton1991planning}{Sutton, 1991b}
\bibcite{sutton1996generalization}{Sutton, 1996}
\bibcite{sutton1998reinforcement}{Sutton and Barto, 1998}
\bibcite{sutton2012dyna}{Sutton et\nobreakspace  {}al., 2012}
\bibcite{thorndike1933proof}{Thorndike, 1933}
\bibcite{tolman1948cognitive}{Tolman, 1948}
\bibcite{whiteson2006evolutionary}{Whiteson and Stone, 2006}
\bibcite{wiering2012reinforcement}{Wiering and Van\nobreakspace  {}Otterlo, 2012}
\bibstyle{apalike}
