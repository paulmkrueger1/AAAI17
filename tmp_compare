diff --git a/model-free-model-based-shaping.tex b/model-free-model-based-shaping.tex
index f46ca3e..3f9b8b7 100644
--- a/model-free-model-based-shaping.tex
+++ b/model-free-model-based-shaping.tex
@@ -38,15 +38,15 @@ Model-free and model-based reinforcement learning have provided a successful fra
 
 \section{Introduction}
 
-The problem of learning from environmental rewards has been studied extensively in both psychology and artificial intelligence research. Both fields have explored two different approaches to solving this problem, known as model-free and model-based reinforcement learning. Model-free learning relies on direct trial-and-error interaction with the environment~\cite{sutton1992reinforcement}, while model-based learning leverages knowledge about the causal structure of the environment~\cite{barto1995learning}. Historically, animal psychologists viewed these two systems as distinct and competing hypotheses, with behaviorists arguing in favor of reflexive, model-free learning based on stimulus-response associations~\cite{thorndike1933proof}, and Tolman and others positing an internal representation of the environment, known as the ``cognitive map"~\cite{tolman1948cognitive}.
+The problem of learning from environmental rewards has been studied extensively in both psychology and artificial intelligence research. Both fields have explored two different approaches to solving this problem, known as model-free and model-based reinforcement learning (RL). Model-free learning relies on direct trial-and-error interaction with the environment~\cite{sutton1992reinforcement}, while model-based learning leverages knowledge about the causal structure of the environment~\cite{barto1995learning}. Historically, animal psychologists viewed these two systems as distinct and competing hypotheses, with behaviorists arguing in favor of reflexive, model-free learning based on stimulus-response associations~\cite{thorndike1933proof}, and Tolman and others positing an internal representation of the environment, known as the "cognitive map"~\cite{tolman1948cognitive}.
 
-Nowadays, while both behavioral and neural data indicate that human learning relies on both systems~\cite{daw2005uncertainty, glascher2010states, dayan2014model}, it is typically assumed that they only compete for control of behavior. However, it is also possible for both systems to cooperate. The Dyna architecture achieves such cooperation by integrating model-free learning with model-based planning~\cite{sutton1991dyna}. In Dyna, as model-free learning occurs, transitions between states of the environment and the resulting rewards are stored in a model. That model is used to replay these past experiences, using them to further train model-free state-action values. Recent behavioral data from people performing a retrospective revaluation task is consistent with a cooperative architecture like Dyna~\cite{gershman2014retrospective}.
+Nowadays, while both behavioral and neural data indicate that human learning relies on both systems~\cite{daw2005uncertainty, glascher2010states, dayan2014model}, it is typically assumed that they only compete for control of behavior. However, it is also possible for both systems to cooperate. The Dyna architecture achieves such cooperation by integrating model-free learning with model-based planning~\cite{sutton1991dyna}. In Dyna, as model-free learning occurs, state-action transitions and reward contingencies are stored in a model, which simultaneously replays these past experiences, using them to further train model-free state-action values. Recent behavioral data with humans performing a retrospective revaluation task is consistent with a cooperative architecture like Dyna~\cite{gershman2014retrospective}.
 
-Here we introduce a new method for cooperative interaction between model-free and model-based learning. The model-based system generates pseudorewards that are used to shape the reward function used in model-free learning. According to the \textit{shaping theorem}, conditions exist under which the optimal decision policy will remain invariant to such modifications of the reward function, opening the possibility that pseudorewards can be used to guide agents toward optimal behavior~\cite{ng1999policy}. That is, if the optimal policy can be guaranteed to remain unchanged with the introduction of pseudorewards, then pseudorewards can potentially be used to guide the agent to the optimal policy. Using these principles, we show that pseudorewards can be used to link model-free and model-based learning through modification of the reward function.
+Here we introduce a new method for cooperative interaction between model-free and model-based learning. The model-based system generates pseudorewards that are used to shape the model-free reward function. According to the \textit{shaping theorem}, conditions exist under which the optimal policy for a Markov Decision Process will remain invariant to such modifications of the reward function, opening the possibility that pseudorewards can be used to guide agents toward optimal behavior~\cite{ng1999policy}. That is, if the optimal policy can be guaranteed to remain unchanged with the introduction of pseudorewards, then pseudorewards can potentially be used to guide the agent to the optimal policy. Using these principles, we show that pseudorewards can be used to link model-free and model-based learning through modification of the reward function.
 
-This method of cooperation between learning systems offers an appealing alternative to Dyna both conceptually and practically. The model-based replay of past experience used to refine state-action values in Dyna suggests that planning (by internal simulation) is one way that different learning systems might be linked in human cognition. The method that we introduce offers an alternative approach, based on changing the reward function. One way that this link may manifest in human cognition is through model-based production of emotions that function as pseudorewards for model-free learning. In addition to providing a new way to think about interaction between reinforcement learning systems, our method also offers a practical advantages over Dyna by learning in fewer steps and requiring less computation time.
+This method of cooperation between learning systems offers an appealing alternative to Dyna both conceptually and practically. Dyna uses model-based replay of past experience to refine state-action values, suggesting the analog in human cognition is planning which links the two systems. Our method introduced here links the two systems by shaping the reward function. One way that this link may manifest in cognition is through model-based production of emotions that function as pseudorewards for model-free learning. In addition to providing a new way to think about interaction between RL systems, our method also offers a practical advantages over Dyna by learning in fewer steps and requiring less computation time.
 
-We begin by reviewing the Dyna architecture for integrated model-free and model-based learning. We then introduce our method and the theoretical background on which it is based. We present two experiments which show the effectiveness of ourmethod, and how it compares to Dyna. The first experiment involves learning in a maze environment, and the second experiment uses the classic mountain car problem. We end by discussing additional variants of Dyna and our method, and consider how this integrated approach might provide a useful model for understanding human cognition and emotion.
+We begin by reviewing the Dyna architecture for integrated model-free and model-based learning. We then introduce our method and the theoretical background on which it is based. We present two experiments which show the effectiveness of our model, and how it compares to Dyna. The first experiment involves learning in a maze environment, and the second experiment uses the classic mountain car problem. We end by discussing additional variants of Dyna and our method, and consider how this integrated approach might provide a useful model for understanding human cognition and emotion.
 
 \section{Cooperative Reinforcement Learning}
 
@@ -63,13 +63,11 @@ An \textit{optimal policy}, $\pi^{*}$, is a policy that maximizes the value func
 
 \subsection{Model-free Reinforcement Learning}
 
-Reinforcement learning (RL) is concerned with learning an effective policy from rewards alone. Model-free methods require no knowledge about the environment, and the agent learns which state-action pairs lead to reward through trial-and-error. One of the most common model-free methods, which is employed throughout the experiments in this paper, is Q-learning~\cite{sutton1998reinforcement}. When the agent takes action $a$ from state $s$, leading to state $s'$ and reward $R(s,s')$, a value $Q(s,a)$ is learned via the update
+RL is concerned with finding a policy that maximizes expected reward. Model-free methods require no knowledge about the environment, and the agent learns which state-action pairs lead to reward through trial-and-error. One of the most common model-free methods, which is employed throughout the experiments in this paper, is Q-learning~\cite{sutton1998reinforcement}. When the agent takes action $a$ from state $s$, leading to state $s'$ and reward $R(s,s')$, a value $Q(s,a)$ is learned via the following algorithm:
 
-\begin{small}
 \begin{equation}
-Q(s,a) \leftarrow Q(s,a) + \alpha (R(s, s') + \gamma \max_{a'} Q(s', a') - Q(s,a)),
+Q(s,a) \leftarrow Q(s,a) + \alpha (R(s, s') + \gamma \max_{a'} Q(s', a') - Q(s,a))
 \end{equation}
-\end{small}
 
 \noindent
 where $\alpha$ is the \textit{learning rate} that determines how quickly the agent learns from new experience. The terms, $R(s, s') + \gamma \max_{a'} Q(s', a') - Q(s,a)$, are called the \textit{temporal difference error}. Q-learning can eventually learn an optimal policy $\pi^*$ over time. Initially all $Q(s,a)$ are zero. The agent uses a \textit{decision policy}, such as the $\epsilon$-greedy policy which is used in our experiments. At each state $s$, with probability $1 - \epsilon$, the agent chooses the action $a \in \mathcal{A}$ with the highest value $Q(s,a)$. With probability $\epsilon$ it chooses an action uniformly at random ($\epsilon$ is a hyperparameter that calibrates the explore-exploit tradeoff).
@@ -79,14 +77,14 @@ where $\alpha$ is the \textit{learning rate} that determines how quickly the age
 Unlike model-free RL, model-based RL has (at least some) knowledge of the environment in terms of the transition probabilities between states, $\mathcal{P}$, and the reward contingencies for state-action pairs, $\mathcal{R}$. One of the most common model-based methods for finding an optimal policy $\pi^*$ is \textit{dynamic programming} which calculates the value of state $s$ under policy $\pi$ according to the \textit{Bellman equation}:
 
 \begin{equation}
-V^{\pi}(s) = R_{\pi(s)}(s,\pi(s)) + \gamma \sum\limits_{s'} P_{\pi(s)}(s,s')V^{\pi}(s')),
+V^{\pi}(s) = R_{\pi(s)}(s,\pi(s)) + \gamma \sum\limits_{s'} P_{\pi(s)}(s,s')V^{\pi}(s'))
 \end{equation}
 
 \noindent
 and finds the value of each state $V^{*}(s)$ under the optimal policy $\pi^*$ by recursively updating these values using the \textit{Bellman optimality equation}:
 
 \begin{equation}
-V^{*}(s) = \max_a R_{\pi(s)}(s,a) + \gamma \sum\limits_{s'} P_{\pi(s)}(s,s')V^{*}(s')).
+V^{*}(s) = \max_a R_{\pi(s)}(s,a) + \gamma \sum\limits_{s'} P_{\pi(s)}(s,s')V^{*}(s'))
 \end{equation}
 
 \subsection{Dyna}
@@ -96,11 +94,11 @@ Dyna uses model-free learning combined with a model-based system that replays pa
 \begin{figure}[ht]
 \centering
 \includegraphics[width=0.4\textwidth]{dyna_schematic}
-\caption{Schematic of the Dyna archetecture.}
+\caption{Schematic of the Dyna archetecture}
 \label{fig:dyna_schematic}
 \end{figure}
 
-In addition to being a useful algorithm for integrating direct learning with indirect replay, Dyna has been proposed as a model of human cognition. ~\cite{gershman2014retrospective} found behavioral evidence in humans cositent with a Dyna architecture.  Participants performed a sequential decision task with separate phases that tested behavioral revaluatation. When given either more time between phases of learning or a smaller cognitive load, the magnitude of revaluation was larger, consistent with model-based replay of past experience. There are also neurophysiological data that suggest Dyna-like cooperation between the two systems. ~\cite{lansink2009hippocampus} identified neurons in the hippocampus of rats encoding spatial location and neurons in the striatum of the same rats that encoded reward. During sleep, the activation of those hippocampal cells correlated with and proceeded activation of the same striatal cells that encoded the value of those locations.
+In addition to being a useful artificial intelligence agent for integrating direct learning with indirect replay, Dyna has begun to provide a useful model of human cognition. ~\cite{gershman2014retrospective} found behavioral evidence in humans cositent with a Dyna architecture.  Participants performed a sequential decision task with separate phases that tested behavioral revaluatation. When given either more time between phases of learning or a smaller cognitive load, the magnitude of revaluation was larger, consistent with model-based replay of past experience. There are also neurophysiological data that suggest Dyna-like cooperation between the two systems. ~\cite{lansink2009hippocampus} identified neurons in the hippocampus of rats encoding spatial location and neurons in the striatum of the same rats that encoded reward. During sleep, the activation of those hippocampal cells correlated with and proceeded activation of the same striatal cells that encoded the value of those locations.
 
 \section{Model-based pseudoreward approximation}
 
@@ -113,17 +111,17 @@ Pseudorewards offer a way of conferring extra information to an agent about the
 R'(s, s') = R(s, s') + F(s, s').
 \end{equation} 
 
-Pseudorewards are defined using \textit{shaping functions}, $F$.  In~\cite{ng1999policy}, conditions for which the optimal policy $\pi^*$ remains invariant under a shaping function are developed. In particular, $F$ must be a potential-based shaping function to possess this invariance property:
+Pseudorewards are defined using \textit{shaping functions}, $F$.  In~\cite{ng1999policy}, conditions for which the optimal policy $\pi^*$ remains invariant under a \textit{shaping function} are developed. In particular, $F$ must be a potential-based shaping function to possess this invariance property:
 
 \begin{equation}
-F(s, a,s') = \gamma \Phi(s') - \Phi(s) ,
+F(s, a,s') = \gamma \Phi(s') - \Phi(s) 
 \end{equation}
 
 \noindent
 where $\Phi$ is a real-valued function, $\Phi : \mathcal{S} \rightarrow \mathbb{R}$. If the shaping function is not potential-based, it is possible that Q-learning will converge to a suboptimal solution. The simplest example of invariant pseudorewards uses the difference in optimal values between the agent's current state and next state:
 
 \begin{equation}
-F(s, s') = \gamma V_{\pi^*}(s') - V_{\pi^*}(s) .
+F(s, s') = \gamma V_{\pi^*}(s') - V_{\pi^*}(s) 
 \end{equation}
 
 This method is called the \textit{optimal policy pseudoreward}--it encourages the agent to always move down the optimal path from its current state. With an $\epsilon$-greedy decision policy, if $\epsilon = 0$, the agent would move directly to the goal along the shortest path.
@@ -141,7 +139,7 @@ Figure~\ref{fig:approxPR_schematic} provides a schematic illustration of how dyn
 \begin{figure}[ht]
 \centering
 \includegraphics[width=0.4\textwidth]{approxPR_schematic}
-\caption{Schematic of the approximate pseudoreward shaping method.}
+\caption{Schematic of the approximate pseudoreward shaping method}
 \label{fig:approxPR_schematic}
 \end{figure}
 
@@ -149,39 +147,33 @@ Figure~\ref{fig:approxPR_schematic} provides a schematic illustration of how dyn
 
 \subsection{Methods}
 
-Our first experiment involved an agent learning in a maze environment~\cite{sutton1991dyna, sutton1991planning, peng1993efficient, sutton1998reinforcement, wiering2012reinforcement}. The agent (a simple Q-learner), began each episode in the upper-left corner of a maze, and was rewarded one point for reaching the lower-right corner (Figure~\ref{fig:maze_values}). The state space consisted of 121 locations in the grid shown in Figure~\ref{fig:maze_values}, and actions consisted of each of the four cardinal directions. The agent was trained for fifty episodes, with each episode ending when the goal was reached, or 2,000 steps were taken (whichever came first). An $\epsilon$-greedy decision policy was used with $\epsilon = 0.25$. The colors in Figure~\ref{fig:maze_values} correspond to state values under the optimal policy. Since rewards were discounted with $\gamma = 0.95$, the value of each state is $0.95^{\min{d}}$, where $d$ is the number of steps to the goal. All simulations were run one-hundred times and averaged.
+Our first experiment entailed an agent learning in a maze environment~\cite{sutton1991dyna, sutton1991planning, peng1993efficient, sutton1998reinforcement, wiering2012reinforcement}. The agent (a simple Q-learner), began each episode in the upper-left corner of a maze, and was rewarded one point for reaching the lower-right corner (Figure~\ref{fig:maze_values}). The state space consisted of 121 locations in the grid shown in Figure~\ref{fig:maze_values}, and actions consisted of each of the four cardinal directions. The agent was trained for fifty episodes, with each episode ending when the goal was reached, or 2,000 steps were taken (whichever came first). An $\epsilon$-greedy decision policy was used with $\epsilon = 0.25$. The colors in Figure~\ref{fig:maze_values} correspond to state values under the optimal policy. Since rewards were discounted with $\gamma = 0.95$, the value of each state is $0.95^{\min{d}}$, where $d$ is the number of steps to the goal. All simulations were run one-hundred times and averaged to produce plots.
 
 \begin{figure}[ht]
 \centering
 \includegraphics[width=0.5\textwidth]{maze_values}
-\caption{Maze environment. Colors correspond to state values under the optimal policy, S is start state, and G is goal state.}
+\caption{Maze environment. Colors correspond to state values under the optimal policy. (S=start state, G=goal state)}
 \label{fig:maze_values}
 \end{figure}
 
 \noindent
-\textbf{Approximate pseudorewards} Dynamic programing was used to approximate state values by iterating over the Bellman equation. In~\cite{mcmahan2005bounded} conditions are defined under which initial state values will provably converge monotonically toward optimal values, but they note that in practice most reasonable initial values will achieve this monotonic convergence. Here, all states were initialized with a lower bound of zero and an upper bound of one, which, in our simple environment, is known to bound state values. Figure~\ref{fig:value_bounds} shows that the approximate state values for each state do indeed converge monotonically. The point at which each state reaches its optimal value is exactly equal to the minimum number of steps that state is from the goal. At each state, the pseudoreward for each action was calculated according to the shaping theorem as the difference between the value of the current state and the value of the of the next state given that action (which was deterministic in this environment). 
+\textbf{Approximate pseudorewards} Dynamic programing was used to approximate state values by iterating over the Bellman equation. In~\cite{mcmahan2005bounded} conditions are defined under which initial state values will provably converge monotonically toward optimal values, but they note that in practice most reasonable initial values will achieve this monotonic convergence. Here, all states were initialized with a lower bound of zero and an upper bound of one, which, in our simple environment, is known to bound state values. Figure~\ref{fig:value_bounds} shows that the approximate state values for each state do indeed converge monotonically. The point at which each state reaches its optimal value is exactly equal to the minimum number of steps that state is from the goal.
 
 \begin{figure}[ht]
 \centering
 \includegraphics[width=0.45\textwidth]{value_bounds_labeled}
-\caption{Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. Red lines are upper bound estimate, blue lines lower bound estimate, and dashed lines optimal state values.}
+\caption{Monotonic convergence of estimated state values. Each subplot corresponds to a state in the maze. (Red lines=upper bound estimate, blue lines=lower bound estimate, dashed lines=optimal state values)}
 \label{fig:value_bounds}
 \end{figure}
 
-
+At each state, the pseudoreward for each action was calculated according to the shaping theorem as the difference between the value of the current state and the value of the of the next state given that action (which was deterministic in this environment). Pseudorewards are added onto actual rewards to shape the reward function, thereby guiding the Q-learning algorithm.
 
 \noindent
-\textbf{Trading off model-free and model-based computation} The closer pseudorewards are to their optimal values, the easier the learning for the model-free agent (at least to some precision). However, whereas Q-learning is simple and quick, the model-based method of approximating state values is relatively slow and computationally costly. Therefore, we sought to understand the most efficient tradeoff between model-based pseudoreward approximation and model-free learning. This was done by computing the amount of CPU time (in seconds) required for each algorithm
-%(while CPU time is a variable factor, the relative time across algorithms is generally invariant).
+\textbf{Trading off model-free and model-based computation} The closer pseudorewards are to their optimal values, the easier the learning for the model-free agent (at least to some precision). However, whereas Q-learning is simple and quick, the model-based method of approximating state values is relatively slow and computationally costly. Therefore, we sought to understand the most efficient tradeoff between model-based pseudoreward approximation and model-free learning. This was done by computing the amount of CPU time (in seconds) required for each algorithm (while CPU time is a variable factor, the relative time across algorithms is generally invariant).
 
 \subsection{Results}
 
-Either the lower-bound of state values or the upper-bound after $n$ iterations of Bellman updates can be used to approximate pseudorewards for Q-learning, according to the shaping theorem. Figure~\ref{fig:maze1} shows the number of steps per episode needed to reach the goal, averaged across 50 episodes, as a function of the the number of Bellman updates used to approximate pseudorewards.
-%The red line shows learning when approximate pseudorewards are based on upper-bound state values, and the blue line is for lower-bound values.
-As expected, learning is quicker when pseudorewards are closer to their optimal values. For comparison, we also show performance of the Dyna algorithm, as a function of the number of planning steps taken after each real step.
-%(separate horizontal axis in green).
-While approximate pseudorewards are calculated just once using $n$ iterations, the planning steps used by Dyna are taken \textit{after every single step of every episode}.
-%The dashed green lines show the number of real steps and the number of planning steps taken using Dyna.
+Either the lower-bound of state values or the upper-bound after $n$ iterations of Bellman updates can be used to approximate pseudorewards for Q-learning, according to the shaping theorem. Figure~\ref{fig:maze1} shows the number of steps per episode needed to reach the goal, averaged across 50 episodes, as a function of the the number of Bellman updates used to approximate pseudorewards. The red line shows learning when approximate pseudorewards are based on upper-bound state values, and the blue line is for lower-bound values. As expected, learning is quicker when pseudorewards are closer to their optimal values. For comparison, we also show performance of the Dyna algorithm, as a function of the number of planning steps taken after each real step (separate horizontal axis in green). While approximate pseudorewards are calculated just once using $n$ iterations, the planning steps used by Dyna are taken \textit{after every single step of every episode}. The dashed green lines show the number of real steps and the number of planning steps taken using Dyna.
 
 Because Dyna must learn a model of the environment through experience, the first episodes require more than the minimal number of steps to learn the goal, which is why the average for real steps only is higher than the shortest path (34 steps when $\epsilon=0.25$). With sufficiently precise pseudorewards, on the other hand, the agent can learn the shortest path on the very first episode. Specifically, 24 Bellman updates are required for this, because the start state is 24 steps away from the goal state; after 24 iterations of the Bellman equation, optimal state values have propagated back from the goal state to the start state, along the shortest path.
 
@@ -207,17 +199,14 @@ Next, we calculated the actual time (in CPU seconds) required to learn the short
 
 \subsection{Methods}
 
-Experiment 2 explored learning in a classic mountain car environment~\cite{moore1990efficient, sutton1996generalization, sutton1998reinforcement, smart2000practical, rasmussen2003gaussian, whiteson2006evolutionary, heidrich2008variable, sutton2012dyna}. The agent begins in a valley between two mountains with the goal of reaching the top of the mountain on the right. The agent must learn to apply force such that it oscillates between the slopes of each mountain, building enough momentum until it finally reaches the top. States consisted of discretized locations along the 2-D mountainsides and discretized velocities. Actions consisted of discretized forces applied tangentially to the direction of movement. The agent used Q-learning during 200 learning episodes, where each episode ended when the car reached the goal state (or if 1,000 steps were taken). When the agent reached the goal it was conferred a reward of one (all other states had zero reward). An $\epsilon$-greedy decision policy was used where $\epsilon=0.01\times0.99^{i-1}$, where $i$ is the episode number. As before, all simulations were run one-hundred times and averaged together.
+Experiment 2 entailed learning in a classic mountain car environment~\cite{moore1990efficient, sutton1996generalization, sutton1998reinforcement, smart2000practical, rasmussen2003gaussian, whiteson2006evolutionary, heidrich2008variable, sutton2012dyna}. The agent begins in a valley between two mountains with the goal of reaching the top of the mountain on the right. The agent must learn to apply force such that it oscillates between the slopes of each mountain, building enough momentum until it finally reaches the top. States consisted of discretized locations along the 2-D mountainsides and discretized velocities. Actions consisted of discretized forces applied tangentially to the direction of movement. The agent used Q-learning during 200 learning episodes, where each episode ended when the car reached the goal state (or if 1,000 steps were taken). When the agent reached the goal it was conferred a reward of one (all other states had zero reward). An $\epsilon$-greedy decision policy was used where $\epsilon=0.01\times0.99^{i-1}$, where $i$ is the episode number. As before, all simulations were run one-hundred times and averaged together.
 
 \noindent
 \textbf{Comparison of learning methods} As before, pseudorewards were approximated using bounded dynamic programming and the shaping theorem. Performance using this algorithm was compared with Dyna.
 
 \subsection{Results}
 
-Figure~\ref{fig:mc1} shows the number of steps per episodes required to reach the goal, averaged across 200 episodes. Although not shown, the upper-bound and lower-bound estimates of state values all converged to optimal values after 73 iterations of the Bellman equation. 73 is the number of steps required to reach the goal from the starting state under the optimal policy, and thus after 73 Bellman updates, values from the goal state had propagated back to the start state (in this task the start state is the furthest possible state away from the goal).
-%The x-axis shows the number of Bellman updates used to estimate pseudorewards. As before, the blue line indicates pseudorewards based on the lower-bound estimation of state values, and the red line is for the upper-bound approximation of state values. The green lines show the performance of Dyna.
-The total number of steps (real steps plus simulated steps) far exceeds the number of steps using our method, and even the number of actual steps alone does not converge as low as the number of steps taken using our method.
-%The blue asterisk indicates performance of a simple Q-learning agent.
+Figure~\ref{fig:mc1} shows the number of steps per episodes required to reach the goal, averaged across 200 episodes. Although not shown, the upper-bound and lower-bound estimates of state values all converged to optimal values after 73 iterations of the Bellman equation. 73 is the number of steps required to reach the goal from the starting state under the optimal policy, and thus after 73 Bellman updates, values from the goal state had propagated back to the start state (in this task the start state is the furthest possible state away from the goal). The x-axis shows the number of Bellman updates used to estimate pseudorewards. As before, the blue line indicates pseudorewards based on the lower-bound estimation of state values, and the red line is for the upper-bound approximation of state values. The green lines show the performance of Dyna. The total number of steps (real steps plus simulated steps) far exceeds the number of steps using our method, and even the number of actual steps alone does not converge as low as the number of steps taken using our method. The blue asterisk indicates performance of a simple Q-learning agent.
 
 \begin{figure}[t]
 \centering
@@ -241,11 +230,11 @@ We have introduced a new method for cooperatively integrating model-free and mod
 
 \subsection{Equalizing knowledge}
 
-One important difference between our method and Dyna is that Dyna learns the model of the environment, whereas our model is omniscient (that is, it is given the full state-action transition matrix and reward-action pairs). This may at first make any comparison between the two methods in terms of learning or computation time seem unfair. In actuality, however, if Dyna is given a full model so that it is equally omniscient, learning and and computation performance improves only modestly in the maze environment, and becomes worse in the mountain car task (these results are included in the Supplementary Material). An omniscient Dyna agent will only learn more quickly during the first episode, before it has discovered the location of reward. Once it knows how to reach the goal, a full model is unnecessary, and would even slow learning through planning, because unvisited states that do not help the agent reach the goal will be replayed, instead of focusing on useful states. One modification that would save computation time for Dyna would be to modulate the number of planning steps in proportion to the change in variance of estimated Q-values. When the temporal difference errors on average are larger, more planning steps are needed, but as they converge, the number of planning steps goes down.
+One important difference between our method and Dyna is that Dyna learns the model of the environment, whereas our model is omniscient (that is, it is given the full state-action transition matrix and reward-action pairs). This may at first make any comparison between the two methods in terms of learning or computation time seem unfair. In actuality, however, if Dyna is given a full model so that it is equally omniscient, learning and and computation performance improves only modestly in the maze environment, and becomes worse in the mountain car task (these results are included in the Supplementary Material). An omniscient Dyna agent will only learn more quickly during the first episode, before it has discovered the location of reward. Once it knows how to reach the goal, a full model is unnecessary, and would even slow learning through planning, because unvisited states that do not help the agent reach the goal will be replayed, instead of focusing on useful states.One modification that would save computation time for Dyna would be to modulate the number of planning steps in proportion to the change in variance of estimated Q-values. When the temporal difference errors on average are larger, more planning steps are needed, but as they converge, the number of planning steps would go down.
 
 \subsection{Learning the model}
 
-Another way to make our method more comparable to Dyna is to have it learn its model. While our method is omniscient with respect to state-action transitions and rewards, this need not be the case. Our method can also be initialized with a naive model that has a uniform prior for all transition probabilities and reward outcomes (that is, any action is assumed to transition to any other state with equal probability and the expected reward for any action is $R/|{\cal S}|$, where $R$ is the expected reward for performing the task and $|{\cal S}|$ is the number of states in the environment). This model can be used for state value approximation with bounded RTDP, just as before, and as experience is acquired, the model can be updated (with either probabilistic transition estimates or deterministic ones if the environment is assumed to be such). When our method is run this way, it still outperforms Dyna in terms of steps required to learn, although it requires more CPU time since all planning steps use Bellman updates that require slow matrix multiplication (these results are also included in the Supplementary Material).
+Another way to make our method more comparable to Dyna is to have it learn its model. While our method is omniscient with respect to state-action transitions and rewards, this need not be the case. Our method can also be initialized with a naive model that has a uniform prior for all transition probabilities and reward outcomes (that is, any action is assumed to transition to any other state with equal probability and the expected reward for any action is $R/j$, where $R$ is the expected reward for performing the task, and $j$ is the number of states in the environment). This model can be used for state value approximation with bounded RTDP, just as before, and as experience is acquired, the model can be updated (with either probabilistic transition estimates or deterministic ones if the environment is assumed to be such). When our method is run this way, it still outperforms Dyna in terms of steps required to learn, although it requires more CPU time since all planning steps use Bellman updates that require slow matrix multiplication (these results are also included in the Supplement).
 
 \subsection{Prioritized sweeping}
 
@@ -255,7 +244,7 @@ Another improvement to Dyna is known as prioritized sweeping~\cite{moore1993prio
 
 By providing a new way to link model-free and model-based RL, our method offers a new way to think about human cognition, and to potentially test through experiments. While cooperative reinforcement learning in humans is only starting to be investigated behaviorally, there is substantial interest in understanding the interactions between them~\cite{daw2014algorithmic}. As discussed earlier, Dyna is readily likened to planning in human cognition as a means to train a model-free system. What might be an analog of pseudoreward approximation in human cognition? For any given task or goal-directed behavior, emotions quite often have the effect of altering the reward landscape, and it is reasonable to think of them as pseudorewards. If certain emotions represent the values of states that are stored in a model, and these emotions are used to train model-free learning by adding bonuses (positive emotions) or punishments (negative emotions) to certain actions, this would be quite akin to our method. The accuracy with which the emotion represents the value of a state would depend on the accuracy of the model, and could be implemented using Bellman approximation or something similar.
 
-The method we have introduced links the two systems cooperatively by shaping the reward function. One way that this link may manifest in cognition is in the domain of moral decision making, where model-based production of emotions function as pseudorewards for model-free learning and decision making. It has been suggested that the dual-system approach to moral psychology is well described by the distinction between model-free and model-based RL, with the former describing the emotional, action-oriented system and the later mapping onto the rational, cognitive, and outcome-oriented system ~\cite{cushman2013action, crockett2013models}. Our method may provide a direct link between these two systems, with the model-based cognitive system producing particular emotions that function as pseudorewards, shaping the model-free emotional system. For example, when one's moral behavior deviates from one's understanding of ethics, leading to an untoward outcome, remorse could be generated to correct the action-oriented propensity that produced the misguided behavior. It is worth pursuing these questions experimentally to test the utility of our method for understanding human cognition and emotion.
+Our method introduced here links the two systems cooperatively by shaping the reward function. One way that this link may manifest in cognition is in the domain of moral decision making, where model-based production of emotions function as pseudorewards for model-free learning and decision making. It has been suggested that the dual-system approach to moral psychology is well described by the distinction between model-free and model-based RL, with the former describing the emotional, action-oriented system and the later mapping onto the rational, cognitive, and outcome-oriented system ~\cite{cushman2013action, crockett2013models}. Our method may provide a direct link between these two systems, with the model-based cognitive system producing particular emotions that function as pseudorewards, shaping the model-free emotional system. For example, when one's moral behavior deviates from one's understanding of ethics, leading to an untoward outcome, remorse could be generated to correct the action-oriented propensity that produced the misguided behavior. It is worth pursuing these questions experimentally to test the utility of our method for understanding human cognition and emotion.
 
 \bibliography{model-free-model-based-shaping}{}
 \bibliographystyle{apalike}
diff --git a/model-free-model-based-shaping_supplementary.tex b/model-free-model-based-shaping_supplementary.tex
index 20ed63d..f92ee94 100644
--- a/model-free-model-based-shaping_supplementary.tex
+++ b/model-free-model-based-shaping_supplementary.tex
@@ -43,7 +43,7 @@ Whereas our method is given a full model of the environment (i.e. all state-acti
 
 The omniscient Dyna agent learns considerably quicker than the non-omniscient agent (Figure~\ref{fig:S1a}; cf. Figure 5 in the main text), but still requires more steps (both real steps and planning steps) than the pseudoreward agent. Similarly, the CPU time required to learn the shortest path is less for the omniscient Dyna agent, although still slightly slower than the pseudoreward agent (Figure~\ref{fig:S1b}; cf. Figure 6 in the main text).
 
-%\textbf{Maze learning}
+\textbf{Maze learning}
 
 \begin{figure}[ht]
 \centering
@@ -98,11 +98,11 @@ Once again, the performance improved modestly for the omniscient Dyna agent, but
 
 \subsection{Methods}
 
-Another way to make the pseudoreward agent more comparable to a standard Dyna agent is to have it learn a model of the environment. Just like the Dyna agent, it learns the model by storing state-action transitions and reward outcomes. Initially, it has a uniform prior overall state-action transitions and rewards, such that the probability of transitioning from any state, $s$, to any other state (or staying in the same state), $s'$, is $1/|{\cal S}|$ where $|{\cal S}|$ is the number of states in the environment, and the expected reward for any such transition is $R/|{\cal S}|$, where $R$ is the expected reward for performing the task (1 for both the maze environment and the mountain car environment). Whenever the agent takes an action it replaces transition probabilities and rewards based on its experience (which is deterministic in our evironments, but this could easily be generalized to nondeterministic environments). That is, the transition probability from state $s$ to state $s'$ for action $a$ becomes one, and zero for all other states. This prior corresponds to a bimodal stick function, with sticks at zero and one. After each real step in the environment, the pseudoreward agent performs $n$ Bellman updates using its current model of the environment (or, it may perform less than $n$ iterations, based on an epsilon-optimal policy with use of span for the stopping criterion, but in practice the bound is usually $n$ after the first couple episodes of learning). These estimated state values are used to update pseudorewards after each real step.
+Another way to make the pseudoreward agent more comparable to a standard Dyna agent is to have it learn a model of the environment. Just like the Dyna agent, it learns the model by storing state-action transitions and reward outcomes. Initially, it has a uniform prior overall state-action transitions and rewards, such that the probability of transitioning from any state, $s$, to any other state (or staying in the same state), $s'$, is $1/j$ where $j$ is the number of states in the environment, and the expected reward for any such transition is $R/j$, where $R$ is the expected reward for performing the task (1 for both the maze environment and the mountain car environment). Whenever the agent takes an action it replaces transition probabilities and rewards based on its experience (which is deterministic in our evironments, but this could easily be generalized to nondeterministic environments). That is, the transition probability from state $s$ to state $s'$ for action $a$ becomes one, and zero for all other states. This prior corresponds to a bimodal stick function, with sticks at zero and one. After each real step in the environment, the pseudoreward agent performs $n$ Bellman updates using its current model of the environment (or, it may perform less than $n$ iterations, based on an epsilon-optimal policy with use of span for the stopping criterion, but in practice the bound is usually $n$ after the first couple episodes of learning). These estimated state values are used to update pseudorewards after each real step.
 
 \subsection{Results}
 
-%\textbf{Maze learning}
+\textbf{Maze learning}
 
 The number of steps required for learning is much higher in the psedoreward agent that learns a model of the environment, due to taking $n$ planning steps after each real step (Figure~\ref{fig:3a}; cf Figure 5 in the main text). However, learning still requires significantly fewer steps than than a standard (non-omniscient) Dyna agent. In terms of CPU time, on the other hand, the model-learning pseudoreward agent takes much longer to learn than the standard Dyna agent (Figure~\ref{fig:S3b}; cf. Figure 6 in the main text). This is because performing Bellman updates (requiring matrix multiplication) is about two orders of magnitude slower than performing Q-learning updates (requiring scalar multiplication only). Since $n$ iterations of the Bellman equation take place \textit{after every real step}, this computation becomes quite costly.
 
